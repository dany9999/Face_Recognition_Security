{"cells":[{"cell_type":"markdown","metadata":{},"source":["# SENet"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","import torch\n","\n","\n","def load_state_dict(model, fname):\n","    \"\"\"\n","\n","    Arguments:\n","        model: model\n","        fname: file name of parameters converted from a Caffe model, assuming the file format is Pickle.\n","    \"\"\"\n","    with open(fname, 'rb') as f:\n","        weights = pickle.load(f, encoding='latin1')\n","\n","    own_state = model.state_dict()\n","    for name, param in weights.items():\n","        if name in own_state:\n","            try:\n","                own_state[name].copy_(torch.from_numpy(param))\n","            except Exception:\n","                raise RuntimeError('While copying the parameter named {}, whose dimensions in the model are {} and whose '\\\n","                                   'dimensions in the checkpoint are {}.'.format(name, own_state[name].size(), param.size()))\n","        else:\n","            raise KeyError('unexpected key \"{}\" in state_dict'.format(name))"]},{"cell_type":"markdown","metadata":{},"source":["# Testing NN2 (SENet)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from SENet import SENet\n","from utils import get_labels\n","model = SENet.senet50(num_classes=8631, include_top=True)\n","\n","load_state_dict(model,'senet50_scratch_weight.pkl')\n","model.eval()\n","\n","LABELS = get_labels()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import tensorflow as tf\n","import re\n","from torchvision import transforms\n","from utils import load_image_NN2\n","\n","dataset_dir =  \"test_set_cropped/\"\n","\n","correct_predictions = 0\n","total_images = 0\n","\n","for filename in os.listdir(dataset_dir):\n","    if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\"):\n","        person_path = os.path.join(dataset_dir, filename)\n","        img_test = load_image_NN2(person_path)\n","        with torch.no_grad():\n","            preds = model(img_test)\n","        \n","        pred_id = torch.argmax(preds, dim=1).item()\n","\n","        pred_label = LABELS[pred_id]\n","\n","        print(\"Immagine:\", filename)\n","        print(\"Predizione del modello:\", pred_label)\n","        correct_label = re.sub(r'_\\d+_face_0\\.jpg$', '', filename)\n","        print(\"Etichetta corretta:\", correct_label)\n","\n","\n","        if correct_label in pred_label:\n","                correct_predictions += 1\n","                print(\"OK\")\n","        '''\n","        else:\n","             with open('predizioni_errate.txt', 'a') as file:\n","              file.write(filename)\n","              file.write('\\n')\n","        '''\n","        total_images += 1\n","        print(\"-------------------------->\")\n","\n","\n","if total_images == 0:\n","    print(\"Nessuna immagine trovata nel dataset.\")\n","else:\n","    test_accuracy = correct_predictions / total_images\n","    print('\\n')\n","    print('\\n')\n","    print(\"Test Accuracy:\", test_accuracy)\n","    print(\"Numero di predizioni corrette:\", correct_predictions)\n","    print(\"Numero di immagini processate:\", total_images)"]},{"cell_type":"markdown","metadata":{},"source":["# Single immage test"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from PIL import Image\n","import numpy as np\n","import requests\n","import os\n","import torchvision.transforms as transforms\n","\n","\n","\n","\n","# Carica l'immagine\n","img = Image.open(\"test_set_cropped/Antonio_Cassano_10_face_0.jpg\")\n","\n","# Ridimensiona l'immagine a 224x224\n","img = img.resize((224, 224))\n","\n","img = add_padding(img, (224,224))\n","\n","# Converti l'immagine in un array NumPy\n","img = np.array(img)\n","\n","# Mean BGR values\n","mean_bgr = [91.4953, 103.8827, 131.0912]\n","\n","# Applica le trasformazioni\n","\n","\n","img_tensor = transform(img, mean_bgr)\n","img_tensor = img_tensor.unsqueeze(0)  # Aggiungi una dimensione batch\n","\n","# Imposta il modello in modalit√† di valutazione\n","model.eval()\n","\n","# Esegui l'inferenza\n","with torch.no_grad():\n","    preds = model(img_tensor)\n","\n","pred_id = torch.argmax(preds, dim=1).item()\n","\n","pred_label = LABELS[pred_id]\n","\n","print('Predicted: ', pred_label)\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMsWduAdraG+GSPW7/QfF9Z","collapsed_sections":["N_cw7xfJ8EMh","y5OeeqXE76fq","sL4KIDLs8r9l","BxZzsqAD89_d"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}
