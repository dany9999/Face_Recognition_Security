{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99TPPK-yRues",
        "outputId": "dd1ab4e8-7aa4-41b0-950b-1132ce531ab1"
      },
      "outputs": [],
      "source": [
        "!pip install facenet-pytorch\n",
        "!pip install Pillow\n",
        "!pip install -q tensorflow==2.0.0\n",
        "!pip install adversarial-robustness-toolbox[all]\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4BN4c2mSO_C",
        "outputId": "490779df-993c-4981-8f06-bf1d89f60dcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /var/folders/zn/72wpk2m96vxbs3ywgd2lm2440000gn/T/ipykernel_17565/3965431075.py:15: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "GPU Available:  False\n"
          ]
        }
      ],
      "source": [
        "# LIBRERIE UTILI\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import art\n",
        "\n",
        "if tf.__version__[0] != '2':\n",
        "    raise ImportError('This notebook requires TensorFlow v2.')\n",
        "\n",
        "print(\"GPU Available: \", tf.test.is_gpu_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from facenet_pytorch import InceptionResnetV1\n",
        "\n",
        "resnet = InceptionResnetV1(pretrained='vggface2').eval()\n",
        "resnet.classify = True\n",
        "\n",
        "\n",
        "\n",
        "fpath = tf.keras.utils.get_file('rcmalli_vggface_labels_v2.npy',\n",
        "                             \"https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_labels_v2.npy\",\n",
        "                             cache_subdir=\"./\")\n",
        "LABELS = np.load(fpath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJYzBVyoIHpG"
      },
      "source": [
        "# **FSGM ATTACK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "B1eYAPDsVp81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 160, 160])\n",
            "<built-in method size of Tensor object at 0x32887ea90>\n",
            "torch.Size([1, 3, 160, 160])\n",
            "<built-in method size of Tensor object at 0x3286c1680>\n",
            "(1, 3, 160, 160)\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "# Import the attack\n",
        "from art.attacks.evasion import FastGradientMethod\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from art.estimators.classification import PyTorchClassifier\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "def load_image(filename):\n",
        "    img = Image.open(filename)\n",
        "    rsz = img.resize((160, 160))\n",
        "    tns = transforms.ToTensor()(rsz)\n",
        "    return tns\n",
        "\n",
        "model = PyTorchClassifier(resnet,input_shape=[224,224], loss=CrossEntropyLoss(),nb_classes=8631) #This class implements a classifier with the PyTorch framework.\n",
        "\n",
        "test_img = load_image(\"test_set_cropped/Antonio_Cassano_8_face_0.jpg\")\n",
        "\n",
        "print(test_img.shape)\n",
        "print(test_img.size)\n",
        "test_img = test_img.unsqueeze(0)\n",
        "print(test_img.shape)\n",
        "print(test_img.size)\n",
        "test_img = test_img.numpy()\n",
        "print(test_img.shape)\n",
        "print(type(test_img))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCHPt-zrIaS0"
      },
      "source": [
        "**NON TARGETED**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "su6gs65jIB1T",
        "outputId": "df9e2cd2-9d42-4bcd-deef-af265b37c0c1"
      },
      "outputs": [],
      "source": [
        "#FSGM generic Attack for single sample\n",
        "\n",
        "epsilon = 0.0005\n",
        "attack = FastGradientMethod(estimator=model, eps=epsilon, targeted=False)\n",
        "batch_size = test_img.shape[0]\n",
        "targeted_labels = np.array([2] * batch_size)   #Al posto di 2 ci va l'indice della label della persona\n",
        "one_hot_targeted_labels = tf.keras.utils.to_categorical(targeted_labels, num_classes=8631)   #Creazione attacco\n",
        "test_images_adv = attack.generate(test_img)                                 # Generazione campione avversario\n",
        "model_predictions = model.predict(test_images_adv)\n",
        "loss = model.compute_loss(test_images_adv,one_hot_targeted_labels)                          # Predizione\n",
        "perturbation = np.mean(np.abs((test_images_adv - test_img)))\n",
        "predicted_label = LABELS[np.array(model_predictions[0].argmax())]\n",
        "print(\"{} con probabilità {}\".format(predicted_label,model_predictions[0][model_predictions.argmax()]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "dataset_dir = \"test_set_cropped\"\n",
        "\n",
        "#FSGM generic Attack for all samples\n",
        "\n",
        "correct_predictions = 0\n",
        "total_images = 0\n",
        "eps_range = [0.001, 0.005, 0.007, 0.01, 0.05, 0.07, 0.1, 0.5, 0.7]\n",
        "accuracy_plot = []\n",
        "for epsilon in eps_range:\n",
        "    correct_predictions = 0\n",
        "    total_images = 0\n",
        "    attack = FastGradientMethod(estimator=model, eps=epsilon, targeted=False)\n",
        "\n",
        "    for filename in os.listdir(dataset_dir):\n",
        "        if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\"):\n",
        "            person_path = os.path.join(dataset_dir, filename)\n",
        "            test_img = load_image(person_path)\n",
        "            test_img = test_img.unsqueeze(0)\n",
        "            test_img = test_img.numpy()\n",
        "            test_images_adv = attack.generate(test_img)\n",
        "            model_predictions = model.predict(test_images_adv)\n",
        "            correct_label = re.sub(r'_\\d+_face_0\\.jpg$', '', filename)\n",
        "            print(\"Etichetta corretta:\", correct_label)   \n",
        "            perturbation = np.mean(np.abs((test_images_adv - test_img)))\n",
        "            predicted_label = LABELS[np.array(model_predictions[0].argmax())]\n",
        "            print(\"Predetto {} con probabilità {}\".format(predicted_label,model_predictions[0][model_predictions.argmax()]))\n",
        "            total_images+=1\n",
        "            \n",
        "            predicted_label = str(predicted_label)\n",
        "\n",
        "            if correct_label in predicted_label:\n",
        "                correct_predictions+=1\n",
        "\n",
        "            accuracy = correct_predictions/total_images\n",
        "            print(\"Accuracy sugli adversarial Sample: {}%\".format((100-(accuracy*100))))\n",
        "        \n",
        "\n",
        "    if total_images != 0:\n",
        "        final_accuracy = correct_predictions/total_images\n",
        "        accuracy_plot.append(final_accuracy)\n",
        "        print(\"----------- Accuracy FINALE sugli adversarial Sample: {}\\% ----------------\".format(final_accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(np.array(eps_range), np.array(accuracy_plot), 'b--', label='NN1')\n",
        "\n",
        "legend = ax.legend(loc='upper center', shadow=True, fontsize='large')\n",
        "legend.get_frame().set_facecolor('#00FFCC')\n",
        "\n",
        "plt.xlabel('Attack strength (eps)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTA: da inserire --> FSGM specific Attack for all samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma5f4kouI2DL"
      },
      "source": [
        "**TARGETED ATTACK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wNm-9QLyI7-V"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average perturbation: 9628.52\n",
            "Etichetta target: Angelica_Celaya\n",
            "[[-1693.0969  -3693.3835   1458.5857  ...  -293.34073   704.0545\n",
            "    680.75305]]\n",
            " Daniele_Bonera con probabilità 10355.302734375\n"
          ]
        }
      ],
      "source": [
        "#FSGM specif Attack for single sample\n",
        "\n",
        "target_class = 555\n",
        "epsilon = 10000\n",
        "attack = FastGradientMethod(estimator=model, eps=epsilon, targeted=True)\n",
        "\n",
        "# Trasformazione etichetta categorica\n",
        "targeted_labels = target_class*np.ones(LABELS.size)\n",
        "one_hot_targeted_labels = tf.keras.utils.to_categorical(targeted_labels, num_classes = 8631)\n",
        "test_images_adv = attack.generate(test_img, one_hot_targeted_labels)\n",
        "\n",
        "#loss_test, accuracy_test = model.evaluate(test_images_adv, test_labels)\n",
        "#print('Accuracy on adversarial test data: {:4.2f}%'.format(accuracy_test * 100))\n",
        "model_predictions = model.predict(test_images_adv)\n",
        "perturbation = np.mean(np.abs((test_images_adv - test_img)))\n",
        "print('Average perturbation: {:4.2f}'.format(perturbation))\n",
        "#targeted_attack_loss, targeted_attack_accuracy = model.evaluate(test_images_adv, targeted_labels)\n",
        "#print('Targeted attack accuracy: {:4.2f}'.format(targeted_attack_accuracy))\n",
        "print(\"Etichetta target:{}\".format(LABELS[target_class]))\n",
        "print(model_predictions)\n",
        "predicted_label = LABELS[np.array(model_predictions.argmax())]\n",
        "print(\"{} con probabilità {}\".format(predicted_label,model_predictions[0][model_predictions.argmax()]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwQhy67oJgRg"
      },
      "source": [
        "# **PGD ATTACK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36aFPpGHOI54"
      },
      "source": [
        "https://adversarial-robustness-toolbox.readthedocs.io/en/latest/modules/attacks/evasion.html#fast-gradient-method-fgm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2NgNjeB2JlnC"
      },
      "outputs": [],
      "source": [
        "# Import attack\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from art.attacks.evasion import ProjectedGradientDescentPyTorch\n",
        "from art.estimators.classification import PyTorchClassifier\n",
        "\n",
        "#Impostare l'input shape\n",
        "classifier = PyTorchClassifier(resnet,input_shape=[224,224], loss=CrossEntropyLoss(),nb_classes=8631) #This class implements a classifier with the PyTorch framework.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3_1-Q9JJt_q"
      },
      "source": [
        "**NON-TARGETED ATTACK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "epsilon = 0.1\n",
        "eps_step = 0.1\n",
        "max_iter = 1 \n",
        "\n",
        "attack = ProjectedGradientDescentPyTorch(estimator=classifier, eps = epsilon, eps_step=eps_step, targeted=False, max_iter = max_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AEgG-h3vOBqM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Alfie_Allen con probabilità 8.514165878295898\n"
          ]
        }
      ],
      "source": [
        "# PGD generic Attack for single sample\n",
        "\n",
        "test_images_adv = attack.generate(test_img)  # Utilizzare generate per generare i campion\n",
        "model_predictions = classifier.predict(test_images_adv) # Classifier\n",
        "predicted_label = LABELS[np.array(model_predictions.argmax())] \n",
        "print(\"{} con probabilità {}\".format(predicted_label,model_predictions[0][model_predictions.argmax()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "dataset_dir = \"test_set_cropped/\"\n",
        "\n",
        "# PGD generic Attack for all samples\n",
        "\n",
        "\n",
        "correct_predictions = 0\n",
        "total_images = 0\n",
        "for filename in os.listdir(dataset_dir):\n",
        "    if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\"):\n",
        "        person_path = os.path.join(dataset_dir, filename)\n",
        "        print(\"Immagine:\", filename)\n",
        "        test_img = load_image(person_path)\n",
        "        test_img = test_img.unsqueeze(0)\n",
        "        test_img = test_img.numpy()\n",
        "        test_images_adv = attack.generate(test_img)\n",
        "        model_predictions = model.predict(test_images_adv)\n",
        "        correct_label = re.sub(r'_\\d+_face_0\\.jpg$', '', filename)\n",
        "        print(\"Etichetta corretta:\", correct_label)   \n",
        "        perturbation = np.mean(np.abs((test_images_adv - test_img)))\n",
        "        predicted_label = LABELS[np.array(model_predictions[0].argmax())]\n",
        "        print(\"Predetto {} con probabilità {}\".format(predicted_label,model_predictions[0][model_predictions.argmax()]))\n",
        "        total_images+=1\n",
        "\n",
        "        if predicted_label == correct_label:\n",
        "            correct_predictions+=1\n",
        "\n",
        "        accuracy = correct_predictions/total_images\n",
        "        print(\"Accuracy sugli adversarial Sample: {}%\".format((100-(accuracy*100))))\n",
        "        \n",
        "\n",
        "if total_images != 0:\n",
        "    final_accuracy = correct_predictions/total_images\n",
        "    print(\"----------- Accuracy FINALE sugli adversarial Sample: {}\\% ----------------\".format(final_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyfbCXWTJ8_E"
      },
      "source": [
        "**TARGETED ATTACK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "epsilon = 1\n",
        "eps_step = 0.1\n",
        "max_iter = 1 \n",
        "\n",
        "attack = ProjectedGradientDescentPyTorch(estimator=classifier, eps = epsilon, eps_step= eps_step, targeted=True, max_iter = max_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jcsAMKtxJs11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 8631)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average perturbation: 0.10\n",
            "Etichetta target: Aaron_Hernandez\n",
            "[[ 1.2787857   2.1339145   0.04435374 ... -3.9367952  -0.46481484\n",
            "   0.7138224 ]]\n",
            " Hassan_Nasrallah con probabilità 10.409660339355469\n"
          ]
        }
      ],
      "source": [
        "#PGD specific Attack for single sample\n",
        "\n",
        "target_class = 10 \n",
        "\n",
        "batch_size = test_img.shape[0]\n",
        "targeted_labels = np.array([target_class] * batch_size)\n",
        "one_hot_targeted_labels = tf.keras.utils.to_categorical(targeted_labels, num_classes=8631)\n",
        "\n",
        "print(one_hot_targeted_labels.shape)\n",
        "test_images_adv = attack.generate(test_img, one_hot_targeted_labels)\n",
        "\n",
        "\n",
        "model_predictions = model.predict(test_images_adv)\n",
        "perturbation = np.mean(np.abs((test_images_adv - test_img)))\n",
        "print('Average perturbation: {:4.2f}'.format(perturbation))\n",
        "#targeted_attack_loss, targeted_attack_accuracy = model.evaluate(test_images_adv, targeted_labels)\n",
        "#print('Targeted attack accuracy: {:4.2f}'.format(targeted_attack_accuracy))\n",
        "print(\"Etichetta target:{}\".format(LABELS[target_class]))\n",
        "print(model_predictions)\n",
        "predicted_label = LABELS[np.array(model_predictions.argmax())]\n",
        "print(\"{} con probabilità {}\".format(predicted_label,model_predictions[0][model_predictions.argmax()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5BFqC2sKAAR"
      },
      "outputs": [],
      "source": [
        "#PGD specific Attack for all samples\n",
        "\n",
        "correct_predictions = 0\n",
        "total_images = 0\n",
        "target_class = 10\n",
        "print(\"ETICHETTA TARGET: \", LABELS[10])\n",
        "batch_size = test_img.shape[0]\n",
        "targeted_labels = np.array([target_class] * batch_size)\n",
        "one_hot_targeted_labels = tf.keras.utils.to_categorical(targeted_labels, num_classes=8631)\n",
        "\n",
        "for filename in os.listdir(dataset_dir):\n",
        "    if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\"):\n",
        "        person_path = os.path.join(dataset_dir, filename)\n",
        "        print(\"Immagine:\", filename)\n",
        "        test_img = load_image(person_path)\n",
        "        test_img = test_img.unsqueeze(0)\n",
        "        test_img = test_img.numpy()\n",
        "        test_images_adv = attack.generate(test_img, one_hot_targeted_labels)\n",
        "        model_predictions = model.predict(test_images_adv)\n",
        "        correct_label = re.sub(r'_\\d+_face_0\\.jpg$', '', filename)\n",
        "        print(\"Etichetta corretta:\", correct_label)   \n",
        "        perturbation = np.mean(np.abs((test_images_adv - test_img)))\n",
        "        predicted_label = LABELS[np.array(model_predictions[0].argmax())]\n",
        "        print(\"Predetto {} con probabilità {}\".format(predicted_label,model_predictions[0][model_predictions.argmax()]))\n",
        "        total_images+=1\n",
        "\n",
        "        if predicted_label == correct_label:\n",
        "            correct_predictions+=1\n",
        "\n",
        "        accuracy = correct_predictions/total_images\n",
        "        print(\"Accuracy sugli adversarial Sample: {}%\".format((100-(accuracy*100))))\n",
        "        \n",
        "\n",
        "if total_images != 0:\n",
        "    final_accuracy = correct_predictions/total_images\n",
        "    print(\"----------- Accuracy FINALE sugli adversarial Sample: {}\\% ----------------\".format(final_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX5rYHS-KHF7"
      },
      "source": [
        "#  **CARLINI WAGNER **ATTACK****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wBtMFh-xKPQd"
      },
      "outputs": [],
      "source": [
        "# Import all L-distance based attacks\n",
        "from art.attacks.evasion import CarliniL2Method, CarliniL0Method, CarliniLInfMethod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g59hEEgPKJZX"
      },
      "source": [
        "**NON-TARGETED ATTACK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "binary_search_steps = 1\n",
        "confidence = 0.5\n",
        "max_iter = 10\n",
        "learning_rate = 0.01\n",
        "initial_const = 1000\n",
        "\n",
        "attack = CarliniL2Method(classifier=classifier, binary_search_steps=binary_search_steps, confidence=confidence, max_iter=max_iter, learning_rate=learning_rate, initial_const=initial_const, targeted=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "x2SLv9tcKNKc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C&W L_2: 100%|██████████| 1/1 [00:03<00:00,  3.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average perturbation: 0.03\n",
            " Aaron_Hernandez\n",
            "[[ 2.5345778  -0.17539069  4.2408724  ... -1.8134753   0.874722\n",
            "  -2.5158045 ]]\n",
            " Antonio_Orozco con probabilità 12.907977104187012\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Carlini Wagner generic attack on single sample\n",
        "\n",
        "test_images_adv = attack.generate(test_img)\n",
        "\n",
        "#loss_test, accuracy_test = model.evaluate(test_images_adv, test_labels)\n",
        "model_predictions = model.predict(test_images_adv)\n",
        "perturbation = np.mean(np.abs((test_images_adv - test_img)))\n",
        "#print('Accuracy on adversarial test data: {:4.2f}%'.format(accuracy_test * 100))\n",
        "print('Average perturbation: {:4.2f}'.format(perturbation))\n",
        "print(LABELS[target_class])\n",
        "print(model_predictions)\n",
        "predicted_label = LABELS[np.array(model_predictions.argmax())]\n",
        "print(\"{} con probabilità {}\".format(predicted_label,model_predictions[0][model_predictions.argmax()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carlini Wagner generic attack on all samples\n",
        "\n",
        "\n",
        "correct_predictions = 0\n",
        "total_images = 0\n",
        "for filename in os.listdir(dataset_dir):\n",
        "    if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\"):\n",
        "        person_path = os.path.join(dataset_dir, filename)\n",
        "        print(\"Immagine:\", filename)\n",
        "        test_img = load_image(person_path)\n",
        "        test_img = test_img.unsqueeze(0)\n",
        "        test_img = test_img.numpy()\n",
        "        test_images_adv = attack.generate(test_img)\n",
        "        model_predictions = model.predict(test_images_adv)\n",
        "        correct_label = re.sub(r'_\\d+_face_0\\.jpg$', '', filename)\n",
        "        print(\"Etichetta corretta:\", correct_label)   \n",
        "        perturbation = np.mean(np.abs((test_images_adv - test_img)))\n",
        "        predicted_label = LABELS[np.array(model_predictions[0].argmax())]\n",
        "        print(\"Predetto {} con probabilità {}\".format(predicted_label,model_predictions[0][model_predictions.argmax()]))\n",
        "        total_images+=1\n",
        "\n",
        "        if predicted_label == correct_label:\n",
        "            correct_predictions+=1\n",
        "\n",
        "        accuracy = correct_predictions/total_images\n",
        "        print(\"Accuracy sugli adversarial Sample: {}%\".format((100-(accuracy*100))))\n",
        "        \n",
        "\n",
        "if total_images != 0:\n",
        "    final_accuracy = correct_predictions/total_images\n",
        "    print(\"----------- Accuracy FINALE sugli adversarial Sample: {}\\% ----------------\".format(final_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLASDM-AKYol"
      },
      "source": [
        "**TARGETED ATTACK**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "binary_search_steps = 1\n",
        "confidence = 0.5\n",
        "max_iter = 10\n",
        "learning_rate = 0.01\n",
        "initial_const = 1000\n",
        "target_class = 6\n",
        "\n",
        "attack = CarliniL2Method(classifier=classifier, binary_search_steps=binary_search_steps, confidence=confidence, max_iter=max_iter, learning_rate=learning_rate, initial_const=initial_const, targeted=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "IVZYhMyVKeVr"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C&W L_2: 100%|██████████| 1/1 [00:02<00:00,  2.58s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average perturbation: 0.00\n",
            " AB_de_Villiers\n",
            "[[ 2.4935617e+00  1.5412122e-03  3.4582591e+00 ... -2.8792291e+00\n",
            "  -2.6755357e-01 -4.0426984e+00]]\n",
            " Paola_Barale con probabilità 13.70297908782959\n"
          ]
        }
      ],
      "source": [
        "# Carlini Wagner specific attack on single sample\n",
        "\n",
        "\n",
        "# Trasformazione del dato categorico\n",
        "targeted_labels = target_class*np.ones(LABELS.size)\n",
        "one_hot_targeted_labels = tf.keras.utils.to_categorical(targeted_labels, num_classes = 8631)\n",
        "test_images_adv = attack.generate(test_img, one_hot_targeted_labels)\n",
        "\n",
        "model_predictions = model.predict(test_images_adv)\n",
        "perturbation = np.mean(np.abs((test_images_adv - test_img)))\n",
        "print('Average perturbation: {:4.2f}'.format(perturbation))\n",
        "#targeted_attack_loss, targeted_attack_accuracy = model.evaluate(test_images_adv, targeted_labels)\n",
        "#print('Targeted attack accuracy: {:4.2f}'.format(targeted_attack_accuracy))\n",
        "print(LABELS[target_class])\n",
        "print(model_predictions)\n",
        "predicted_label = LABELS[np.array(model_predictions.argmax())]\n",
        "print(\"{} con probabilità {}\".format(predicted_label,model_predictions[0][model_predictions.argmax()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C&W L_2: 100%|██████████| 1/1 [00:02<00:00,  2.64s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Immagine: Michael_Phelps_9_face_0.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C&W L_2:   0%|          | 0/1 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m test_img \u001b[38;5;241m=\u001b[39m test_img\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     18\u001b[0m test_img \u001b[38;5;241m=\u001b[39m test_img\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 19\u001b[0m test_images_adv \u001b[38;5;241m=\u001b[39m \u001b[43mattack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_hot_targeted_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m model_predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_images_adv)\n\u001b[1;32m     21\u001b[0m correct_label \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+_face_0\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.jpg$\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, filename)\n",
            "File \u001b[0;32m~/Desktop/AI CyberSecurity/Face_Recognition_Security/.venv/lib/python3.9/site-packages/art/attacks/evasion/carlini.py:335\u001b[0m, in \u001b[0;36mCarliniL2Method.generate\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# compute gradient:\u001b[39;00m\n\u001b[1;32m    334\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompute loss gradient\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 335\u001b[0m perturbation_tanh \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loss_gradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mz_logits\u001b[49m\u001b[43m[\u001b[49m\u001b[43mactive\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mactive\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mactive\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_adv_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mactive\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_adv_batch_tanh\u001b[49m\u001b[43m[\u001b[49m\u001b[43mactive\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc_current\u001b[49m\u001b[43m[\u001b[49m\u001b[43mactive\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_min\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# perform line search to optimize perturbation\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# first, halve the learning rate until perturbation actually decreases the loss:\u001b[39;00m\n\u001b[1;32m    348\u001b[0m prev_loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mcopy()\n",
            "File \u001b[0;32m~/Desktop/AI CyberSecurity/Face_Recognition_Security/.venv/lib/python3.9/site-packages/art/attacks/evasion/carlini.py:214\u001b[0m, in \u001b[0;36mCarliniL2Method._loss_gradient\u001b[0;34m(self, z_logits, target, x, x_adv, x_adv_tanh, c_weight, clip_min, clip_max)\u001b[0m\n\u001b[1;32m    208\u001b[0m     i_add \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(target, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    209\u001b[0m     i_sub \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\n\u001b[1;32m    210\u001b[0m         z_logits \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m target) \u001b[38;5;241m+\u001b[39m (np\u001b[38;5;241m.\u001b[39mmin(z_logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)[:, np\u001b[38;5;241m.\u001b[39mnewaxis] \u001b[38;5;241m*\u001b[39m target,\n\u001b[1;32m    211\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    212\u001b[0m     )\n\u001b[0;32m--> 214\u001b[0m loss_gradient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_adv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi_add\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m loss_gradient \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mclass_gradient(x_adv, label\u001b[38;5;241m=\u001b[39mi_sub)\n\u001b[1;32m    216\u001b[0m loss_gradient \u001b[38;5;241m=\u001b[39m loss_gradient\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
            "File \u001b[0;32m~/Desktop/AI CyberSecurity/Face_Recognition_Security/.venv/lib/python3.9/site-packages/art/estimators/classification/pytorch.py:688\u001b[0m, in \u001b[0;36mPyTorchClassifier.class_gradient\u001b[0;34m(self, x, label, training_mode, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m unique_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(label))\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m unique_label:\n\u001b[0;32m--> 688\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_device\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m grads \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mswapaxes(np\u001b[38;5;241m.\u001b[39marray(grads_list), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    695\u001b[0m lst \u001b[38;5;241m=\u001b[39m [unique_label\u001b[38;5;241m.\u001b[39mindex(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m label]\n",
            "File \u001b[0;32m~/Desktop/AI CyberSecurity/Face_Recognition_Security/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/AI CyberSecurity/Face_Recognition_Security/.venv/lib/python3.9/site-packages/art/estimators/classification/pytorch.py:654\u001b[0m, in \u001b[0;36mPyTorchClassifier.class_gradient.<locals>.save_grad.<locals>.hook\u001b[0;34m(grad)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_grad\u001b[39m():\n\u001b[0;32m--> 654\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhook\u001b[39m(grad):\n\u001b[1;32m    655\u001b[0m         grads_list\u001b[38;5;241m.\u001b[39mappend(grad\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[1;32m    656\u001b[0m         grad\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mzero_()\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Carlini Wagner specific attack on all samples\n",
        "\n",
        "\n",
        "correct_predictions = 0\n",
        "total_images = 0\n",
        "target_class = 10\n",
        "\n",
        "targeted_labels = target_class*np.ones(LABELS.size)\n",
        "one_hot_targeted_labels = tf.keras.utils.to_categorical(targeted_labels, num_classes = 8631)\n",
        "test_images_adv = attack.generate(test_img, one_hot_targeted_labels)\n",
        "\n",
        "for filename in os.listdir(dataset_dir):\n",
        "    if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\"):\n",
        "        person_path = os.path.join(dataset_dir, filename)\n",
        "        print(\"Immagine:\", filename)\n",
        "        test_img = load_image(person_path)\n",
        "        test_img = test_img.unsqueeze(0)\n",
        "        test_img = test_img.numpy()\n",
        "        test_images_adv = attack.generate(test_img, one_hot_targeted_labels)\n",
        "        model_predictions = model.predict(test_images_adv)\n",
        "        correct_label = re.sub(r'_\\d+_face_0\\.jpg$', '', filename)\n",
        "        print(\"Etichetta corretta:\", correct_label)   \n",
        "        perturbation = np.mean(np.abs((test_images_adv - test_img)))\n",
        "        print('Average perturbation: {:4.2f}'.format(perturbation))\n",
        "        predicted_label = LABELS[np.array(model_predictions[0].argmax())]\n",
        "        print(\"Predetto {} con probabilità {}\".format(predicted_label,model_predictions[0][model_predictions.argmax()]))\n",
        "        \n",
        "        total_images+=1\n",
        "\n",
        "        if predicted_label == correct_label:\n",
        "            correct_predictions+=1\n",
        "\n",
        "        accuracy = correct_predictions/total_images\n",
        "        print(\"Accuracy sugli adversarial Sample: {}%\".format((100-(accuracy*100))))\n",
        "        \n",
        "\n",
        "if total_images != 0:\n",
        "    final_accuracy = correct_predictions/total_images\n",
        "    print(\"----------- Accuracy FINALE sugli adversarial Sample: {}\\% ----------------\".format(final_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md2LFzBFKdr7"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjoKBgEzKiT3"
      },
      "source": [
        "# CODICE PLOT IMMAGINI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pBWFWD9Kk8e"
      },
      "outputs": [],
      "source": [
        "#Show one original example\n",
        "plt.figure()\n",
        "plt.matshow(test_images[0])\n",
        "plt.title(\"Original Label: {}\".format(test_labels[0]))\n",
        "plt.show()\n",
        "\n",
        "#Show the corresponding adversarial example\n",
        "plt.figure()\n",
        "plt.matshow(test_images_adv[0])\n",
        "plt.title(\"Model Prediction: {}\".format(np.argmax(model_predictions[0])))\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1b5074f2cdae47e48054916c00395960": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24f78c2c7dc542778022cac4cde42a9e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2622eee74356474db8227019f2e2372f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e84c181de7140f59bd0730b64becb5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_785554c18f4a4db5ae2f91feae8e730f",
              "IPY_MODEL_ca18afa898cc48fa9504321adc1a52cc",
              "IPY_MODEL_8c31d67f7e5f44e595e3f8317f983712"
            ],
            "layout": "IPY_MODEL_494ccdc8390245168c20038628de8f3e"
          }
        },
        "494ccdc8390245168c20038628de8f3e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69dd02bb29604f7fbfb82d32168f3aca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "785554c18f4a4db5ae2f91feae8e730f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24f78c2c7dc542778022cac4cde42a9e",
            "placeholder": "​",
            "style": "IPY_MODEL_bc910750b37a48d0bbf7fa55a0a369fc",
            "value": "100%"
          }
        },
        "85caef33330243728789276dfd0189e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c31d67f7e5f44e595e3f8317f983712": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2622eee74356474db8227019f2e2372f",
            "placeholder": "​",
            "style": "IPY_MODEL_1b5074f2cdae47e48054916c00395960",
            "value": " 107M/107M [00:00&lt;00:00, 232MB/s]"
          }
        },
        "bc910750b37a48d0bbf7fa55a0a369fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca18afa898cc48fa9504321adc1a52cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85caef33330243728789276dfd0189e2",
            "max": 111898327,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_69dd02bb29604f7fbfb82d32168f3aca",
            "value": 111898327
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
